---
title: "AS13B"
author: "第十一組"
date: "`r Sys.time()`"
output: 
  html_document:
    highlight: pygments
    theme: flatly
    css: style.css
---
```{r results='hide', message=FALSE, warning=FALSE, echo=F}
# Formating Codes.  Do not change the codes in this chunk !!
rm(list=ls(all=T))
knitr::opts_chunk$set(comment = NA)
knitr::opts_knit$set(global.par = TRUE)
par(cex=0.8)
options(scipen=20, digits=5, width=80)
if(!require(pacman)) install.packages("pacman")
```
<hr>
### Ans.
```{r results='hide', message=FALSE, warning=FALSE}
pacman::p_load(caTools, ggplot2, dplyr)
D = read.csv("data/quality.csv")  # Read in dataset
```


<p class="qiz">
<span style="font-size:24px">`r "\U1F5FF"` 練習： </span><br>
使用`TR$MemberID`以外的所有欄位，建立一個邏輯式回歸模型來預測`PoorCare`，並：<br>
&emsp; 【A】 分別畫出`Training`和`Testing`的`DPP`<br>
&emsp; 【B】 分別畫出`Training`和`Testing`的`ROC`<br>
&emsp; 【C】 分別算出`Training`和`Testing`的`ACC`、`SENS`和`SPEC`<br>
&emsp; 【D】 分別算出`Training`和`Testing`的`AUC`<br>
&emsp; 【E】 跟用兩個預測變數的模型相比，這一個模型有比較準嗎？<br>
&emsp; 【F】 為什麼它比較準(或比較不準)呢？<br><br>
</p class="qiz">


### 資料設定
```{r}
set.seed(88)
split = sample.split(D$PoorCare, SplitRatio = 0.75)  # split vector
TR = subset(D, split == TRUE)
TS = subset(D, split == FALSE)
glm2 = glm(PoorCare ~ .-MemberID,
           TR,
           family = binomial)
summary(glm2)
```

##### Training Data

**預測機率 Predicted Probability (Training)**
```{r fig.height=3.2, fig.width=6.4}
par(cex=0.8)
pred_1 = predict(glm2, type="response")
hist(pred_1)
abline(v=0.5, col='red')
```

**混淆矩陣 Confusion Matrix (Training)**
```{r}
CMX = table(Acture = TR$PoorCare, predict = pred_1 >0.5)
CMX
```

**模型準確性指標 Accuracy Matrices (Training)**
```{r}
A2x2 = function(x, k=3) c(
  accuracy = sum(diag(x))/sum(x),
  sensitivity = as.numeric(x[2,2]/rowSums(x)[2]),
  specificity = as.numeric(x[1,1]/rowSums(x)[1])
  ) %>% round(k)
A2x2(CMX)
```

##### Testing Data

**預測機率 Predicted Probability (Testing)**
```{r fig.height=3.2, fig.width=6.4}
par(cex=0.8)
pred_2 = predict(glm2, newdata=TS, type="response")
hist(pred_2, 10)
abline(v=0.5, col='red')
```

**混淆矩陣 Confusion Matrix (Testing)**
```{r}
CMX2 = table(Acture=TS$PoorCare, Predict=pred_2 > 0.5)
CMX2
```

**模型準確性指標 Accuracy Matrices (Testing)**
```{r}
sapply(list(Train=CMX, Test=CMX2), A2x2)
```

<br><br><hr>

### 【A】DPP
**預測機率分佈 (DPP) - Distribution of Predicted Probability**
```{r fig.height=3.2, fig.width=7}
data.frame(y=factor(TR$PoorCare), pred=pred_1) %>% 
  ggplot(aes(x=pred_1, fill=y)) + 
  geom_histogram(bins=20, col='white', position="stack", alpha=0.5) +
  ggtitle("Distribution of Predicted Probability (Training DPP)") +
  xlab("predicted probability")
data.frame(y=factor(TS$PoorCare), pred=pred_2) %>% 
  ggplot(aes(x=pred_2, fill=y)) + 
  geom_histogram(bins=20, col='white', position="stack", alpha=0.5) +
  ggtitle("Distribution of Predicted Probability (Testing DPP)") +
  xlab("predicted probability")
```

### 【B】ROC

**ROC - Receiver Operation Curve（left->training, right->testing）**
```{r fig.height=4, fig.width=7.2}
par(mfrow=c(1,2), cex=0.8)
trAUC = colAUC(pred_1, y=TR$PoorCare, plotROC=T)
tsAUC = colAUC(pred_2, y=TS$PoorCare, plotROC=T)
```

###【C】算出ACC、SENS、SPEC

```{r}
A2x2 = function(x, k=3) c(
  ACC = sum(diag(x))/sum(x),
  SENS = as.numeric(x[2,2]/rowSums(x)[2]),
  SPEC = as.numeric(x[1,1]/rowSums(x)[1])
  ) %>% round(k)
sapply(list(Train=CMX, Test=CMX2), A2x2)
```

###【D】AUC

**AUC - Area Under Curve**
```{r}
c(trAUC, tsAUC)
```

###【E】跟用兩個預測變數的模型相比，這一個模型有比較準嗎？

**從ACC做比較（數值愈大愈準）<br>**
而在只有`OfficeVisits`與`Narcotics`兩變數時：<br>
ACC在Training和Testing的值分別為0.808和0.812<br>
而在此模型下：<br>
ACC在Training和Testing的值分別為0.798和0.844<br>
**從AUC做比較（數值愈大愈準）<br>**
而在只有`OfficeVisits`與`Narcotics`兩變數時：<br>
AUC在Training和Testing的值分別為0.77459和0.79948<br>
而在此模型下：<br>
AUC在Training和Testing的值分別為0.87568和0.86458<br>
因此，只有兩變數的模型相較此模型，發生過度適配的情形較明顯，當需要使用模型進行預測時，此模型可能較為準確。


###【F】為什麼它比較準(或比較不準)呢？

**改變變數的數量會影響模型的正確性：<br>**
當變數增加時通常training的ACC會增加，對testing的影響則不一定，可能會產生overfit的情形。<br>
然而，從上面資訊得知當變數增加時Training的ACC卻呈現下降的結果，可以知道當只使用`OfficeVisits`與`Narcotics`兩變數時，模型較可能有過度適配的情形。<br>


<br><br><br><hr>